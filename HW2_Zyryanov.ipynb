{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "corpus = [sent.split() for sent in open('corpus_ng.txt', encoding='utf8').read().splitlines()]\n",
    "WORDS = Counter()\n",
    "for sent in corpus:\n",
    "    WORDS.update(sent)\n",
    "\n",
    "vocab = list(WORDS.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "vec = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "def get_closest_match_vec(text, X, vec, TOPN=5):\n",
    "    v = vec.transform([text])\n",
    "    similarities = cosine_distances(v, X)\n",
    "    topn = similarities.argsort()[0][:TOPN]\n",
    "    \n",
    "    return [id2word[top] for top in topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "\n",
    "def get_closest_match_with_metric(text, lookup, metric=textdistance.levenshtein):\n",
    "    similarities = Counter()\n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_hybrid_match(text, X, vec, metric=textdistance.levenshtein):\n",
    "    vec_matches = get_closest_match_vec(text, X, vec)\n",
    "    final_metric = 0\n",
    "    closest = ''\n",
    "    for el in vec_matches:\n",
    "        temp_metric = metric.normalized_similarity(text, el)\n",
    "        if temp_metric > final_metric:\n",
    "            final_metric = temp_metric\n",
    "            closest = el\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 96 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'апофеоз'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_closest_hybrid_match('апофиоз', X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "punct = set(punctuation)\n",
    "\n",
    "mistakes = []\n",
    "total = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        corr = get_closest_hybrid_match(pair[1], X, vec)\n",
    "        if pair[0] != corr:\n",
    "            newmistake = (pair[0], corr)\n",
    "            mistakes.append(newmistake)\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля ошибок: 0.17059528565721135\n"
     ]
    }
   ],
   "source": [
    "print('Доля ошибок:', len(mistakes)/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('симпатичнейшее', 'пластичнейшими'), ('шпионское', 'шпионские'), ('гламурный', 'лагерный'), ('бонда', 'банд'), ('superheadz', 'sueddeutsche'), ('clap', 'place'), ('camera', 'america'), ('получатся', 'ополчатся'), ('язычки', 'язычка'), ('очень', 'чечни'), ('милые', 'милы'), ('насчет', 'защищено'), ('чавеса', 'чавес'), ('попавшим', 'пропавшим'), ('аварийно-спасательных', 'аварийно-восстановительных')]\n"
     ]
    }
   ],
   "source": [
    "print(mistakes[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что частым типом ошибок является замена слова с опечаткой на слово того же корня, но в неверной грамматической форме (шпионское - шпионскИе) либо на слово другого, но близкого по буквенному составу, корня (получатся-ополчатся)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
